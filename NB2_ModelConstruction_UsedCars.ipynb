{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKrvyhfVL3e"
   },
   "source": [
    "# **Predicting Used Car Prices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Predictive Model Construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aat-Ne-ZVL3e"
   },
   "source": [
    "* **Step1:** Encode the dependent and independent variables \n",
    "    * Price_log is my target or dependent variable, because it's value depends on the value of the feature / indepdent variables.  \n",
    "* **Step2:** Encode the categorical variables in X using pd.dummies. \n",
    "    * Absent Model variable, which has 211 unique values\n",
    "* **Step3:** Split the data into train and test using train_test_split.  \n",
    "    * Test size = 0.3 \n",
    "* **Step 4:** Construct predictive regression models: \n",
    "    * Linear regression model \n",
    "    * Lasso regression model \n",
    "    * Decision Tree \n",
    "        + Hyperparameter tuning \n",
    "    * Random Forest\n",
    "        + Hyperparameter tuning \n",
    "* **Step 5:** Evaluate, compare and identify the best performing modelc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaj2riZFVL3g"
   },
   "source": [
    "## Table of Contents\n",
    "1. [Set up](#setup)\n",
    "2. [Train Test Split](#traintest)\n",
    "3. [Linear Regression](#linear) \n",
    "4. [Lasso Regression](#lasso)\n",
    "5. [Decision Tree](#dtree)  \n",
    "    * [Decision Tree: Hyperparameter Tuning](#dtree-tuning) \n",
    "6. [Random Forest](#randomforest) \n",
    "    * [Random Forest: Hyperparameter Tuning](#rf-tuning) \n",
    "7. [Conclusion](#conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# **1. Set up** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Statsmodels for statistical modeling\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Scikit-learn imports for model selection and evaluation\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error, r2_score, make_scorer \n",
    "from sklearn import metrics\n",
    "\n",
    "# Scikit-learn imports for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scikit-learn imports for linear models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Scikit-learn imports for tree-based models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6018, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Used_cars_final.xlsx') \n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-checking that we have no missing values in our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brand            0\n",
       "Model            0\n",
       "Year             0\n",
       "Owner            0\n",
       "KM_driven_log    0\n",
       "Fuel             0\n",
       "Engine_log       0\n",
       "Power_log        0\n",
       "Mileage_log      0\n",
       "Transmission     0\n",
       "Seats            0\n",
       "Location         0\n",
       "Price_log        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"traintest\"></a>\n",
    "# **2. Train Test Split** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data types:** We have 6 categorical and 7 numerical variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brand             object\n",
       "Model             object\n",
       "Year               int64\n",
       "Owner             object\n",
       "KM_driven_log    float64\n",
       "Fuel              object\n",
       "Engine_log       float64\n",
       "Power_log        float64\n",
       "Mileage_log      float64\n",
       "Transmission      object\n",
       "Seats              int64\n",
       "Location          object\n",
       "Price_log        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of unique values in each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brand              31\n",
       "Model             211\n",
       "Year               22\n",
       "Owner               4\n",
       "KM_driven_log    3092\n",
       "Fuel                5\n",
       "Engine_log        151\n",
       "Power_log         393\n",
       "Mileage_log       440\n",
       "Transmission        2\n",
       "Seats               8\n",
       "Location           11\n",
       "Price_log        1373\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique car Models:  211\n",
      "Number of unique car Brands:  31\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique car Models: ', df['Model'].nunique())  # unique models \n",
    "print('Number of unique car Brands: ', df['Brand'].nunique())  # unique brands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've already applied log_transformation to the numerical variables in the previous notebook. Now, I'm also applying StandardScaler. First, the log transformation deals with skewness, and then the scaling ensures that all features contribute equally to model training, avoiding biases towards features with higher magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have set the random_state parameter is to 42. This is a parameter that controls the randomness of the data shuffling applied to the data before it is split. By setting random_state to a fixed number, ensures that the outcome of the split is reproducible (that the same split will occur every time the code is run).  \n",
    "    * This specific value, 42, doesn't have any special properties aside from being a commonly used number in examples due to its cultural reference (\"the answer to life, the universe, and everything\" in Douglas Adams' \"The Hitchhiker's Guide to the Galaxy\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Price_log', 'Model'], axis=1) # drop the 'Price_log' and 'Model' columns \n",
    "y = df['Price_log'] # target variable\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True) # convert categorical variables to dummy variables \n",
    "\n",
    "# Setting test size to 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model scores function** \n",
    "- In order to avoid repetitive code, I pre-define a function for calculating R2_scores and RMSE on train and test data  \n",
    "- This function takes model as an input on which we have trained particular algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(model, X_train, y_train, X_test, y_test):\n",
    "    # Predict on training and test data\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    # Scores for training set\n",
    "    train_r2 = r2_score(y_train, pred_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, pred_train))\n",
    "\n",
    "    # Scores for test set\n",
    "    test_r2 = r2_score(y_test, pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "\n",
    "    print(f\"R-squared on training set: {train_r2}\")\n",
    "    print(f\"RMSE on training set: {train_rmse}, \\n\")\n",
    "    print(f\"R-squared on test set: {test_r2}\")\n",
    "    print(f\"RMSE on test set: {test_rmse}\")\n",
    "\n",
    "    return train_r2, test_r2, train_rmse, test_rmse  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwL33RaztOS9"
   },
   "source": [
    "<a id=\"linear\"></a>\n",
    "# **3. Linear Regression Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores of the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9366047035414113\n",
      "RMSE on training set: 0.2192064264890547, \n",
      "\n",
      "R-squared on test set: 0.9345409028821385\n",
      "RMSE on test set: 0.2252220071240112\n"
     ]
    }
   ],
   "source": [
    "lr_scores = get_model_scores(lr, X_train, y_train, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Consistency Across Datasets:** The R-squared values for both training (0.9372) and test (0.9319) datasets are quite close, which indicates that the linear model generalizes well on unseen data and does not suffer significantly from overfitting.  \n",
    "* **Error Metrics:** The Root Mean Squared Error (RMSE) values are low (0.2187 on training and 0.2282 on test sets), which implies that on average, the model’s predictions deviate from the actual logarithm of prices by about 0.22. This suggests that the model predictions are relatively precise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinary Least Squares (OLS)**   \n",
    "    * OLS is a method of estimating the parameters of a linear regression model. It specifically minimizes the sum of the squared differences between the observed values and the values predicted by the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              Price_log   R-squared:                       0.937\n",
      "Model:                            OLS   Adj. R-squared:                  0.936\n",
      "Method:                 Least Squares   F-statistic:                     1205.\n",
      "Date:                Tue, 07 May 2024   Prob (F-statistic):               0.00\n",
      "Time:                        12:14:41   Log-Likelihood:                 416.16\n",
      "No. Observations:                4212   AIC:                            -728.3\n",
      "Df Residuals:                    4160   BIC:                            -398.3\n",
      "Df Model:                          51                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                 -221.3663      3.105    -71.299      0.000    -227.453    -215.279\n",
      "Year                     0.1125      0.002     70.746      0.000       0.109       0.116\n",
      "KM_driven_log           -0.0740      0.007    -11.312      0.000      -0.087      -0.061\n",
      "Engine_log               0.3024      0.038      7.934      0.000       0.228       0.377\n",
      "Power_log                0.7492      0.026     28.724      0.000       0.698       0.800\n",
      "Mileage_log             -0.1522      0.031     -4.906      0.000      -0.213      -0.091\n",
      "Seats                    0.0525      0.007      7.619      0.000       0.039       0.066\n",
      "Brand_Audi              -7.5534      0.111    -67.775      0.000      -7.772      -7.335\n",
      "Brand_BMW               -7.5822      0.110    -68.747      0.000      -7.798      -7.366\n",
      "Brand_Bentley           -6.8239      0.239    -28.577      0.000      -7.292      -6.356\n",
      "Brand_Chevrolet         -8.4953      0.113    -75.107      0.000      -8.717      -8.273\n",
      "Brand_Datsun            -8.5060      0.135    -63.085      0.000      -8.770      -8.242\n",
      "Brand_Fiat              -8.3681      0.123    -68.139      0.000      -8.609      -8.127\n",
      "Brand_Force             -8.1899      0.189    -43.436      0.000      -8.560      -7.820\n",
      "Brand_Ford              -8.2459      0.114    -72.311      0.000      -8.469      -8.022\n",
      "Brand_Honda             -8.1942      0.113    -72.822      0.000      -8.415      -7.974\n",
      "Brand_Hyundai           -8.1759      0.113    -72.666      0.000      -8.396      -7.955\n",
      "Brand_ISUZU             -8.5123      0.192    -44.375      0.000      -8.888      -8.136\n",
      "Brand_Isuzu          -1.481e-13   2.11e-15    -70.164      0.000   -1.52e-13   -1.44e-13\n",
      "Brand_Jaguar            -7.4306      0.119    -62.580      0.000      -7.663      -7.198\n",
      "Brand_Jeep              -8.0048      0.129    -61.963      0.000      -8.258      -7.752\n",
      "Brand_Lamborghini       -6.3936      0.245    -26.083      0.000      -6.874      -5.913\n",
      "Brand_Land              -7.2351      0.118    -61.312      0.000      -7.466      -7.004\n",
      "Brand_Mahindra          -8.3526      0.115    -72.678      0.000      -8.578      -8.127\n",
      "Brand_Maruti            -8.1245      0.112    -72.220      0.000      -8.345      -7.904\n",
      "Brand_Mercedes-Benz     -7.5497      0.112    -67.544      0.000      -7.769      -7.331\n",
      "Brand_Mini              -7.2053      0.121    -59.753      0.000      -7.442      -6.969\n",
      "Brand_Mitsubishi        -7.9531      0.123    -64.467      0.000      -8.195      -7.711\n",
      "Brand_Nissan            -8.2029      0.116    -70.444      0.000      -8.431      -7.975\n",
      "Brand_Porsche           -7.4112      0.133    -55.753      0.000      -7.672      -7.151\n",
      "Brand_Renault           -8.2012      0.115    -71.258      0.000      -8.427      -7.976\n",
      "Brand_Skoda             -8.1499      0.113    -72.146      0.000      -8.371      -7.928\n",
      "Brand_Smart           5.893e-14   8.77e-16     67.188      0.000    5.72e-14    6.06e-14\n",
      "Brand_Tata              -8.5971      0.115    -74.879      0.000      -8.822      -8.372\n",
      "Brand_Toyota            -7.9764      0.115    -69.268      0.000      -8.202      -7.751\n",
      "Brand_Volkswagen        -8.1999      0.114    -72.107      0.000      -8.423      -7.977\n",
      "Brand_Volvo             -7.7314      0.125    -62.031      0.000      -7.976      -7.487\n",
      "Owner_Fourth & Above     0.1123      0.084      1.334      0.182      -0.053       0.277\n",
      "Owner_Second            -0.0691      0.010     -6.847      0.000      -0.089      -0.049\n",
      "Owner_Third             -0.1297      0.026     -5.036      0.000      -0.180      -0.079\n",
      "Fuel_Diesel              0.1576      0.038      4.131      0.000       0.083       0.232\n",
      "Fuel_Electric            2.2953      0.247      9.279      0.000       1.810       2.780\n",
      "Fuel_LPG                -0.0014      0.080     -0.018      0.986      -0.158       0.155\n",
      "Fuel_Petrol             -0.0402      0.039     -1.040      0.298      -0.116       0.036\n",
      "Transmission_Manual     -0.0936      0.011     -8.197      0.000      -0.116      -0.071\n",
      "Location_Bangalore       0.1433      0.023      6.315      0.000       0.099       0.188\n",
      "Location_Chennai         0.0237      0.021      1.113      0.266      -0.018       0.066\n",
      "Location_Coimbatore      0.1096      0.021      5.301      0.000       0.069       0.150\n",
      "Location_Delhi          -0.0916      0.021     -4.398      0.000      -0.132      -0.051\n",
      "Location_Hyderabad       0.1175      0.020      5.794      0.000       0.078       0.157\n",
      "Location_Jaipur         -0.0463      0.022     -2.104      0.035      -0.089      -0.003\n",
      "Location_Kochi          -0.0399      0.021     -1.923      0.055      -0.081       0.001\n",
      "Location_Kolkata        -0.2447      0.021    -11.559      0.000      -0.286      -0.203\n",
      "Location_Mumbai         -0.0724      0.020     -3.592      0.000      -0.112      -0.033\n",
      "Location_Pune           -0.0540      0.021     -2.604      0.009      -0.095      -0.013\n",
      "==============================================================================\n",
      "Omnibus:                     1723.333   Durbin-Watson:                   2.033\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            67942.586\n",
      "Skew:                          -1.254   Prob(JB):                         0.00\n",
      "Kurtosis:                      22.515   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.7e-22. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Ensuring the indices are aligned correctly\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Explicitly convert boolean to int if necessary (usually, this should not be required)\n",
    "X_train = X_train.astype(float)\n",
    "\n",
    "x_train_sm = sm.add_constant(X_train) # add a constant to the model data\n",
    "y_train = y_train.astype(float) # ensure no data type issues\n",
    "\n",
    "# Fit the model using statsmodels\n",
    "try:\n",
    "    ols_model = sm.OLS(y_train, x_train_sm).fit()\n",
    "    print(ols_model.summary())\n",
    "except Exception as e:\n",
    "    print(\"Error fitting the model:\", e)\n",
    "    print(\"Check the input data with np.asarray(data).\") \n",
    "    print(x_train_sm.head())\n",
    "    print(y_train.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "* **Categorical Variables:** The model handles categorical variables (like brand and location) well, showing significant differences in price impacts across different categories, which could help stakeholders understand brand and location premiums or discounts in the used car market.\n",
    "* **F-statistic and its Prob (F-statistic):** The F-statistic is very high at 1307, and the probability of the F-statistic is approximately 0.00. This suggests that the model is statistically significant at explaining the variation in Price_log compared to a model with no independent variables.  \n",
    "* **Statistical Significance:** Most predictors are statistically significant as indicated by their p-values (P>|t| nearly 0.00), which suggests these variables have a significant impact on the logarithm of used car prices.  \n",
    "  \n",
    "* **Influence of Features:**  \n",
    "    *  Year, KM_driven_log, Engine_log, Power_log, Mileage_log, and Seats are significant with small p-values, indicating strong evidence against the null hypothesis of no effect.  \n",
    "    * Positive coefficients in the model (e.g., Power_log and Engine_log) suggest that higher engine power and size lead to higher prices. \n",
    "    *  Various brands show different impacts on the price: \n",
    "        * Negative coefficients for brands like Brand_Chevrolet and Brand_Ford might indicate these brands generally fetch lower prices compared to the baseline brand.\n",
    "        * Brand_Bentley and Brand_Lamborghini have a positive impact on price, indicating these are more expensive on average.  \n",
    "    *  Owner_Second and Owner_Third have negative coefficients, suggesting that vehicles with previous owners typically have lower prices.  \n",
    "    *  Fuel types like Diesel and Electric have positive coefficients, suggesting these are associated with higher prices compared to the baseline fuel type (not shown, likely Petrol).  \n",
    "    *  Transmission_Manual has a negative coefficient, indicating manual cars are cheaper compared to automatic ones.  \n",
    "    *  Geographic location also impacts prices. For example, cars in Location_Bangalore fetch higher prices, while those in Location_Kolkata fetch lower prices.\n",
    "    * The coefficient for Year is positive, indicating newer cars tend to have higher prices, which is expected.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNN1nlPqIrdC"
   },
   "source": [
    "**Important variables of the Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive Coeff values, p-values and store them in the dataframe\n",
    "olsmod = pd.DataFrame(ols_model.params, columns = ['coef'])\n",
    "olsmod['pval'] = ols_model.pvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location_Jaipur</th>\n",
       "      <td>-0.046295</td>\n",
       "      <td>3.539471e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Pune</th>\n",
       "      <td>-0.053998</td>\n",
       "      <td>9.238964e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Mumbai</th>\n",
       "      <td>-0.072418</td>\n",
       "      <td>3.317588e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fuel_Diesel</th>\n",
       "      <td>0.157617</td>\n",
       "      <td>3.688641e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Delhi</th>\n",
       "      <td>-0.091592</td>\n",
       "      <td>1.117350e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mileage_log</th>\n",
       "      <td>-0.152170</td>\n",
       "      <td>9.650110e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Owner_Third</th>\n",
       "      <td>-0.129686</td>\n",
       "      <td>4.956927e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Coimbatore</th>\n",
       "      <td>0.109602</td>\n",
       "      <td>1.208168e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Hyderabad</th>\n",
       "      <td>0.117505</td>\n",
       "      <td>7.359720e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Bangalore</th>\n",
       "      <td>0.143263</td>\n",
       "      <td>2.984784e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         coef          pval\n",
       "Location_Jaipur     -0.046295  3.539471e-02\n",
       "Location_Pune       -0.053998  9.238964e-03\n",
       "Location_Mumbai     -0.072418  3.317588e-04\n",
       "Fuel_Diesel          0.157617  3.688641e-05\n",
       "Location_Delhi      -0.091592  1.117350e-05\n",
       "Mileage_log         -0.152170  9.650110e-07\n",
       "Owner_Third         -0.129686  4.956927e-07\n",
       "Location_Coimbatore  0.109602  1.208168e-07\n",
       "Location_Hyderabad   0.117505  7.359720e-09\n",
       "Location_Bangalore   0.143263  2.984784e-10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter by significant p-value (pval <= 0.05) and sort descending by Odds ratio\n",
    "olsmod = olsmod.sort_values(by = \"pval\", ascending = False)\n",
    "pval_filter = olsmod['pval']<= 0.05  \n",
    "olsmod[pval_filter].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8qcI692VL3g"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uubmKLlVL3j"
   },
   "source": [
    "<a id=\"lasso\"></a>\n",
    "# **4. Lasso Regression Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[Lasso regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)** is a type of linear regression that includes a regularization term. Lasso regression aims to minimize the residual sum of squares (like ordinary least squares) plus the sum of the absolute values of the coefficients multiplied by a constant, alpha.\n",
    "\n",
    "* **The alpha parameter** controls the strength of the regularization. A larger alpha means more regularization, which increases the penalty for larger coefficients and can drive some coefficients to zero, effectively performing variable selection. An alpha of 0.01 in our model suggests moderate regularization.\n",
    "* **Max Iterations parameter** set to 50000, specifies the maximum number of iterations for the algorithm to converge to the optimal coefficients. Increasing this value, from 10000 to 50000, helped ensure convergence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize and fit the Lasso regression model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.01, max_iter=50000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.01, max_iter=50000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=50000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01, max_iter=50000)  # increased from 10000 to 50000\n",
    "lasso.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance of the Lasso model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9309619017912658\n",
      "RMSE on training set: 0.22875425097595672, \n",
      "\n",
      "R-squared on test set: 0.9307853387669305\n",
      "RMSE on test set: 0.2315927010503538\n"
     ]
    }
   ],
   "source": [
    "lasso_scores = get_model_scores(lasso, X_train_scaled, y_train, X_test_scaled, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** \n",
    "* **R-squared:** The Lasso model achieves an R-squared of 0.9316 on the training set and 0.9265 on the test set, which are quite high. These values indicate that the model explains a significant portion of the variance in the dependent variable.\n",
    "* **RMSE (Root Mean Squared Error):** The RMSE values are 0.2283 on the training set and 0.2371 on the test set, suggesting that on average, the model’s predictions deviate from the actual values by these amounts (on the logarithmic scale of price, since the dependent variable is Price_log)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dtree\"></a>\n",
    "# **5. Decision Tree Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owyg5IpstOS9"
   },
   "source": [
    "Learn more about Decision Tree models: https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and fit the Decision Tree regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GjZeRER6VL3k"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeRegressor(max_depth=5)  \n",
    "dtree.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree model performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.8595009260455575\n",
      "RMSE on training set: 0.3263333765320665, \n",
      "\n",
      "R-squared on test set: 0.8563424048174433\n",
      "RMSE on test set: 0.3336492209615977\n"
     ]
    }
   ],
   "source": [
    "dtree_scores = get_model_scores(dtree, X_train_scaled, y_train, X_test_scaled, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw0dMSgetOS_"
   },
   "source": [
    "<a id=\"dtree-tuning\"></a>\n",
    "# - Decision Tree: Hyperparameter Tuning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(min_samples_leaf=5, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(min_samples_leaf=5, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(min_samples_leaf=5, random_state=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The type of estimator\n",
    "dtree_tuned = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters \n",
    "parameters = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2']  # removed 'auto'\n",
    "}\n",
    "\n",
    "scorer = make_scorer(r2_score) # type of scoring used to compare parameter combinations\n",
    "\n",
    "# Run the grid search with error_score set to 'raise' for better debugging\n",
    "grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer, cv=5, error_score='raise')\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "dtree_tuned = grid_obj.best_estimator_  # set the clf to the best combination of parameters \n",
    "dtree_tuned.fit(X_train, y_train) # fit the best algorithm to the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model using the generalized function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "shBet9WztOS-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9597444511968335\n",
      "RMSE on training set: 0.17467777025796236, \n",
      "\n",
      "R-squared on test set: 0.9004490790861175\n",
      "RMSE on test set: 0.2777464137475989\n"
     ]
    }
   ],
   "source": [
    "dtree_tuned_scores = get_model_scores(dtree_tuned, X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features of tuned decision tree similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Imp\n",
      "Power_log         0.658116\n",
      "Year              0.232433\n",
      "Engine_log        0.041731\n",
      "Mileage_log       0.015190\n",
      "KM_driven_log     0.012006\n",
      "Brand_Mahindra    0.005094\n",
      "Location_Kolkata  0.004568\n",
      "Brand_Tata        0.004114\n",
      "Brand_Honda       0.003650\n",
      "Seats             0.002919\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with feature importances\n",
    "feature_importances = pd.DataFrame(dtree_tuned.feature_importances_, columns=[\"Imp\"], index=X_train.columns)\n",
    "\n",
    "# Sort the DataFrame by importance in descending order and print the first 20 rows\n",
    "print(feature_importances.sort_values(by='Imp', ascending=False).head(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"randomforest\"></a>\n",
    "# **6. Random Forest Model**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about Random Forest Regressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the Random Forest Regressor and fit on the training data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=1)  \n",
    "rf.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9912164586184354\n",
      "RMSE on training set: 0.0815942565250492, \n",
      "\n",
      "R-squared on test set: 0.9406402841911602\n",
      "RMSE on test set: 0.21447255503980395\n"
     ]
    }
   ],
   "source": [
    "rf_scores = get_model_scores(rf, X_train_scaled, y_train, X_test_scaled, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18uxHTy2tOTB"
   },
   "source": [
    "<a id=\"rf-tuning\"></a>\n",
    "# - Random Forest: Hyperparameter Tuning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_features': ['sqrt', 0.5],  # changed 'auto' to 'sqrt'\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(r2_score)\n",
    "n_iter_search = 10\n",
    "\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=n_iter_search, scoring=scorer, cv=5, random_state=42, n_jobs=-1, \n",
    "                                   error_score='raise', verbose=0)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "rf_tuned = random_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score of the rf_tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9912221781584956\n",
      "RMSE on training set: 0.08156768651216363, \n",
      "\n",
      "R-squared on test set: 0.9460207132932678\n",
      "RMSE on test set: 0.20452169804341983\n"
     ]
    }
   ],
   "source": [
    "rf_tuned_score = get_model_scores(rf_tuned, X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42) # the model\n",
    "\n",
    "# Distribution of parameters to choose from\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': np.arange(2, 21),\n",
    "    'min_samples_leaf': np.arange(1, 21)\n",
    "}\n",
    "\n",
    "# Number of iterations and the scoring function\n",
    "n_iter_search = 20\n",
    "scorer = make_scorer(r2_score)  \n",
    "        \n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=n_iter_search, scoring=scorer, cv=5, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "rf_tuned = random_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on training set: 0.9667341857889861\n",
      "RMSE on training set: 0.15879025367691654, \n",
      "\n",
      "R-squared on test set: 0.9349252813194433\n",
      "RMSE on test set: 0.224559777214033\n"
     ]
    }
   ],
   "source": [
    "rf_tuned_scores = get_model_scores(rf_tuned, X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItsgSUyiIrdI"
   },
   "source": [
    "**Feature Importance**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9khvM2ZhtOTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               importance\n",
      "Power_log        0.659381\n",
      "Year             0.230484\n",
      "Engine_log       0.033509\n",
      "Mileage_log      0.011918\n",
      "KM_driven_log    0.011742\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rf_tuned.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances.head(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# **7. Conclusion** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Train R^2       Test R^2        Train RMSE      Test RMSE      \n",
      "Linear Regression    0.937           0.935           0.219           0.225          \n",
      "Lasso                0.931           0.931           0.229           0.232          \n",
      "Decision Tree        0.860           0.856           0.326           0.334          \n",
      "Tuned Decision Tree  0.960           0.900           0.175           0.278          \n",
      "Random Forest        0.991           0.941           0.082           0.214          \n",
      "Tuned Random Forest  0.967           0.935           0.159           0.225          \n"
     ]
    }
   ],
   "source": [
    "# Store model scores in a dictionary \n",
    "model_scores = {\n",
    "    \"Linear Regression\": lr_scores,\n",
    "    \"Lasso\": lasso_scores,\n",
    "    \"Decision Tree\": dtree_scores, \n",
    "    \"Tuned Decision Tree\": dtree_tuned_scores, \n",
    "    \"Random Forest\": rf_scores,\n",
    "    \"Tuned Random Forest\": rf_tuned_scores\n",
    "}\n",
    "\n",
    "# the headers\n",
    "print(\"{:<20} {:<15} {:<15} {:<15} {:<15}\".format('Model', 'Train R^2', 'Test R^2', 'Train RMSE', 'Test RMSE'))\n",
    "\n",
    "# Print each model's scores in a formatted output\n",
    "for model_name, scores in model_scores.items():\n",
    "    print(\"{:<20} {:<15.3f} {:<15.3f} {:<15.3f} {:<15.3f}\".format(\n",
    "        model_name,\n",
    "        scores[0],  # Train R^2\n",
    "        scores[1],  # Test R^2\n",
    "        scores[2],  # Train RMSE\n",
    "        scores[3]   # Test RMSE\n",
    "    )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression and Lasso:**  \n",
    "* Both models show high and comparable R^2 values on the training and test sets, indicating good generalization. The RMSE values are also quite low, suggesting that the predictions are close to the actual values.  \n",
    "* The slight performance drop from Linear Regression to Lasso could be due to the regularization in Lasso which might be removing some useful predictors due to the shrinkage of coefficients.    \n",
    "\n",
    "**Decision Trees:**\n",
    "* The standard Decision Tree has significantly lower R^2 values and higher RMSE compared to other models, indicating less predictive accuracy and higher errors.  \n",
    "* The Tuned Decision Tree shows improved training metrics drastically (R^2 of 0.96 and RMSE of 0.175), but the increase in Test RMSE to 0.278 suggests some overfitting despite tuning.  \n",
    "\n",
    "**Random Forest:**\n",
    "* The standard Random Forest model shows extremely high R^2 and very low RMSE on the training data, suggesting excellent performance. However, the increase in Test RMSE (though still competitive) hints at some overfitting.  \n",
    "\n",
    "> **Tuned Random Forest:**  \n",
    "* Best General Model  \n",
    "* The Tuned Random Forest has a slightly lower training R^2 than the standard version but a better balance between training and test scores, suggesting less overfitting. \n",
    "* This model provides a strong balance between high R^2 and low RMSE on both training and test datasets. The performance is robust with a relatively small difference between training and test results, indicating good generalization without significant overfitting.\n",
    "\n",
    "Simplicity vs. Performance: If deployment simplicity is a priority (e.g., faster predictions, easier model management), Linear Regression might be a suitable choice as it still provides robust performance metrics and will generally be faster and easier to manage than a complex ensemble model like a Random Forest."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a1WHqIX9tOTC",
    "PBoHEXnjtOTC",
    "TZrq2E9VtOTD"
   ],
   "name": "Reference_Notebook_Milestone_2_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
